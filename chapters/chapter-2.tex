% --------------------------------------------------
% 
% This chapter is for DECO
% 
% --------------------------------------------------


\chapter{Automating the Curation Process of Historical Literature on Marine Biodiversity Using Text Mining: The DECO Workflow}
\label{cha:deco}


\textbf{Citation:} \\ 
Paragkamian Savvas, Sarafidou Georgia, Mavraki Dimitra, Pavloudi Christina, Beja Joana, Eliezer Menashè, Lipizer Marina, Boicenco Laura, Vandepitte Leen, Perez-Perez Ruben, Zafeiropoulos Haris, Arvanitidis Christos, Pafilis Evangelos, Gerovasileiou Vasilis

DOI: \href{https://www.frontiersin.org/articles/10.3389/fmars.2022.940844}\footnote{
   For author contributions and supplementary material please refer to the relevant sections. 
   This is a modified version of the published version,
   in terms of relevance, coherence and formatting.
   }



% PREGO ABSTRACT
\section{Abstract}

Historical biodiversity documents comprise an important link to the long-term data 
life cycle and provide useful insights on several aspects of biodiversity research 
and management. However, because of their historical context, they present 
specific challenges, primarily time- and effort-consuming in data curation. 
The data rescue process requires a multidisciplinary effort involving four tasks: 
(a) Document digitisation (b) Transcription, which involves text recognition and 
correction, and (c) Information Extraction, which is performed using text mining 
tools and involves the entity identification, their normalisation and their 
co-mentions in text. Finally, the extracted data go through (d) Publication to 
a data repository in a standardised format. Each of these tasks requires a 
dedicated multistep methodology with standards and procedures. During the past 
8 years, Information Extraction (IE) tools have undergone remarkable advances, 
which created a landscape of various tools with distinct capabilities specific
to biodiversity data. These tools recognise entities in text such as taxon names, 
localities, phenotypic traits and thus automate, accelerate and facilitate 
the curation process. Furthermore, they assist the normalisation and mapping 
of entities to specific identifiers. This work focuses on the IE step (c) from 
the marine historical biodiversity data perspective. It orchestrates IE tools 
and provides the curators with a unified view of the methodology; as a result 
the documentation of the strengths, limitations and dependencies of several 
tools was drafted. Additionally, the classification of tools into Graphical 
User Interface (web and standalone) applications and Command Line Interface 
ones enables the data curators to select the most suitable tool for their needs, 
according to their specific features. In addition, the high volume of already 
digitised marine documents that await curation is amassed and a demonstration 
of the methodology, with a new scalable, extendable and containerised tool, 
“DECO” (bioDivErsity data Curation programming wOrkflow) is presented. DECO’s 
usage will provide a solid basis for future curation initiatives and an 
augmented degree of reliability towards high value data products that allow 
for the connection between the past and the present, in marine biodiversity research.

% DECO INTRODUCTION
\section{Introduction}
\label{sec:deco-intro}
Species’ occurrence patterns across spatial and temporal scales are the 
cornerstone of ecological research (Levin, 1992). The compilation of both past 
and present marine data to a unified census is crucial to predict the future of 
ocean life (Ausubel, 1999; Anderson, 2006; Lo Brutto, 2021). This compilation has 
been attempted by big collaborative projects, like Census of Marine Life1 (Vermeulen et al., 2013), 
that follow metadata standards and guidelines (Michener et al., 1997; Wilkinson et al., 2016) 
and modern web technologies (Michener, 2015). The project has resulted in the incorporation 
of census data from the past, i.e. historical data, to modern data platforms, such 
as the Ocean Biodiversity Information System (OBIS) (Klein et al., 2019), which feeds 
the Global Biodiversity Information Facility (GBIF) (GBIF, 2022). The transformation of 
historical data to modern standards is necessary for their rescue (data archaeology)
from decay and inevitable loss (Bowker, 2000).

Historical data are usually found in the form of (a) historical literature and 
(b) specimens stored in biodiversity museum collections (Rainbow, 2009) (the 
digital transformation process and progress of specimens is reviewed by Nelson and Ellis, 2019). 
Historical biodiversity documents (also known as legacy, ancient or simply old 
documents) comprise literature from 1000 AD until 1960 and therefore are stored 
in an analogue and/ or obsolete format (Lotze and Worm, 2009; Beja et al., 2022). 
These old documents can be found in institutional libraries, publications, books, 
expedition logbooks, project reports, newspapers (Faulwetter et al., 2016; Mavraki et al., 2016; Kwok, 2017) 
or other types of legacy formats (e.g. stored in floppy disks, microfilms or CDs).

From the scientific point of view, historical biodiversity data are as relevant 
as modern data (Griffin, 2019; Beja et al., 2022). They are valuable for studies on biodiversity loss (Stuart-Smith et al., 2015; Goethem and Zanden, 2021), as forming baseline studies for the design of future samplings (Rivera-Quiroz et al., 2020) and for predictions of future trends (Mouquet et al., 2015). Furthermore, historical data offer the kind of evidence needed for conservation policy and marine resource management, allowing for past patterns and processes to be compared with current ones (Fortibuoni et al., 2010; McClenachan et al., 2012; Costello et al., 2013; Engelhard et al., 2016). Hundreds of historical marine data held in documents have already been uploaded to OBIS, yet a Herculean effort is required to curate the thousands of available documents of the Biodiversity Heritage Library (BHL) (Gwinn and Rinaldo, 2009) and other repositories.

Adequate and interoperable metadata are equally necessary and have to be curated alongside data (Heidorn, 2008; Mouquet et al., 2015). In this context, standards and guidelines have been recently formulated in policies as Findable, Accessible, Interoperable and Reusable (FAIR) (meta)data (Wilkinson et al., 2016; Reiser et al., 2018). Identifiers and semantics are used to accomplish the interoperability and reusability of biodiversity data as well as the monitoring of their use (Mouquet et al., 2015). Indispensable to the curation process of marine data have been the standards of the Biodiversity Information Standards2, more specifically Darwin Core (Wieczorek et al., 2012) and vocabularies such as those included in the International Commission on Zoological Nomenclature3, the World Register of Marine Species4 (WoRMS) (WoRMS Editorial Board, 2022), the Environmental Ontology5 (ENVO) (Buttigieg et al., 2016) and Marine Regions6 (Claus et al., 2014). These standards and vocabularies and their adoption by biodiversity initiatives like GBIF and OBIS align with the goal of marine biodiversity Linked Open Data and support their interoperability and reusability (Page, 2016; Penev et al., 2019; Zárate and Buckle, 2021).

The rescue process of historical biodiversity documents can be summarised in four tasks (Figure 1). The first task is the digitisation of the document, which involves locating and cataloguing the original data sources, imaging/scanning with specific equipment and standards and uploading them to digital libraries (Lin, 2006; Thompson and Richard, 2013). In the second task, the images are analysed with text recognition software, mainly through Optical Character Recognition (OCR) (for standards see Groom et al., 2019 and for reviews see Lyal, 2016 and Owen et al., 2020). Text recognition errors are then corrected manually by professionals or citizen scientists (Herrmann, 2020). The third task is named Information Extraction (IE) as it involves the steps of named entity recognition, mapping and normalisation of biodiversity information (Thessen et al., 2012). Here, the curators may compile a species’ occurrence census enriched with metadata of the study, geolocation, environment, sampling methods and traits among others (Faulwetter et al., 2016). Lastly, the fourth task, is the data publishing to online biodiversity databases/repositories (Costello et al., 2013; Penev et al., 2017). Expert manual curation is a cross-cutting action through all the aforementioned tasks for quality control and stewardship (Vandepitte et al., 2015). This article focuses on the tools and curation procedures encompassed in the third and fourth tasks described above.
   \begin{figure}[h]
      \centering
      \includegraphics[width=0.98\columnwidth]{figures/deco-figure-1.jpg}
      \caption[Historical document rescue process]{
           Summarised process of historical document rescue. Four tasks are required to complete the data rescue process of biodiversity documents. Each of these has several steps, methodology, tools and standards. Curation is needed in every task, for tool handling and error correction. The stars represent the 5-star ranking system of Linked Data as introduced by W3C7 (Heath and Bizer, 2011). Availability of information from historical data increases as the curation tasks are completed (as exemplified by the fan on the right). Icons used from the Noun Project released under CC BY: book by Oleksandr Panasovskyi, scanning by LAFS, Book info by Xinh Studio, Library by ibrandify, Scanner Text by Wolf Böse, Check form by allex, Whale by Alina Oleynik, Fish by Asmuh, tag code vigorn, pivot layout by paisan, Certificate by P Thanga Vignesh, web service by mynamepong.
      }
      \label{fig:deco-rescue-pipeline}
   \end{figure}

   Several factors may turn the curation of historical documents into a serious challenge (Faulwetter et al., 2016; Beja et al., 2022). Errors from the first and second tasks, as presented in Figure 1 \ref{fig:deco-rescue-pipeline} (i.e. bad quality imaging, mis-recognised characters etc.) are propagated through the whole process. In terms of georeferencing constraints, location names or sampling points on an old map may be provided instead of the actual coordinates. Additionally, taxonomic constraints (e.g. old, currently unaccepted synonyms, lack of authority associated with the taxon names) combined with the absence of taxonomic literature or voucher specimens (e.g. identifier number for samples of natural history/expedition collections) require the taxonomists’ assistance. Numerical measurement units often need to be converted to the International System of Units (SI system) (e.g. fathoms to metres) (Calder, 1982; Wieczorek et al., 2012). Old toponyms and political boundaries that have now changed should also be taken into consideration, as well as coordinates that now fall on land instead of in the sea, due to the changes in the coastline. Lastly, the use of languages other than English is quite common in old scientific publications, so multilingual curators are required. Some of the aforementioned issues are presented in Figure 2. Because of these limitations, the manual curation of data and metadata is mandatory when it comes to historical data (Faulwetter et al., 2016).7

Manual curation, a tedious and multistep process, requires substantial effort for the correct interpretation of valuable historical information; however, text mining tools appear to be promising in assisting and accelerating this part of the curation process (Alex et al., 2008). Text mining is the automatic extraction of information from unstructured data (Hearst, 1999; Ananiadou and Mcnaught, 2005). These mining tools build upon standardised knowledge, vocabularies, dictionaries and perform multistep Natural Language Processes. Named Entity Recognition (NER) is a key step in this process for locating terms of interest in text (Perera et al., 2020). The entities of interest for biodiversity documents include: (1) taxon names, (2) people’s names (Page, 2019a; Groom et al., 2020), (3) environments/ habitats (Pafilis et al., 2015; Pafilis et al., 2017), (4) geolocations/ localities (Alex et al., 2015; Stahlman and Sheffield, 2019), (5) phenotypic traits/morphological characteristics (Thessen et al., 2018), (6) physico-chemical variables, and (7) quantities, measurement units and/or values. Subsequent steps include the relation extraction between entities. Multiple tools have emerged to retrieve a single or a collection of these entities in the past few years (Batista-Navarro et al., 2017; Muñoz et al., 2019; Dimitrova et al., 2020; Le Guillarme and Thuiller, 2022).

The work described in this document has a threefold structure: (a) the abundance of marine historical literature digitised/available for curation is attempted to be estimated; (b) bioinformatics tools, focusing on automating and assisting the curation process for these documents, are compiled/reviewed. Two categories of such curation software are described: (i) the first one relies on web and standalone applications with Graphical User Interface (GUI) and the second (ii) combines Command Line Interface (CLI) programming libraries and software packages; lastly, (c) a demonstrator biodiversity data curation workflow, named DECO (bioDivErsity data Curation programming wOrkflow8), developed using programming tools, is presented.

% DECO METHODS
\section{Method}
\label{sec:deco-method}

    \subsection{Historical Literature Discovery}
A search was conducted on BHL to amass the historical literature on BHL regarding marine biodiversity. Using the keywords “marine”, “ocean”, “fishery”, “fisheries” and “sea” on the items’ titles and their subjects (the scripts, results and documentation are available in this repository9) the documents available for information extraction were estimated. Subjects are categories provided for each title and multiple subjects can be assigned to each title. The items that were originally published before 1960 were selected, in order to include only historical documents, according to the definition included in the Introduction section. Furthermore, the taxon names on each page, which were identified by BHL using the Global Names parser tool (Mozzherin et al., 2022), were summarised for every document. Hence, summaries of the number of automatically identified taxon names were calculated along with the page number for each item. Additionally, OBIS’ historical datasets originally published before 1960 were downloaded and analysed. This analysis provides an approximation of the size of available marine historical literature compared to the already rescued documents. All analysis scripts were written in GNU AWK programming language and the visualisation scripts were written in R using the ggplot2 library (Wickham, 2016).


    \subsection{Historical Document Rescue Methodology}
    Data curators thoroughly read each page of a document and insert the data into spreadsheets, mapping them to Darwin Core terms, adding metadata and creating a standard Darwin Core Archive10. This whole process, which is mostly manual,means reading the information (e.g. the occurrence of a specific taxon and its locality) and inputting it through typing to the corresponding cell of the data file. It is, as expected, a time- and resource-consuming procedure. Taxon names, traits, environments and localities can be identified as well and the transformation of these results to database identifiers (IDs), like Life Science Identifier11 (LSID) of Aphia IDs12, Encyclopedia of Life13 (EOL) IDs (Parr et al., 2014), Marine regions gazetteer IDs, marine species traits14 among others, can be facilitated through web applications and programming software. The Natural Environment Research Council15 Vocabulary Server, developed and hosted by the British Oceanographic Data Centre16 was used for mapping facts and additional measurements included in documents.
Tools assist curators in this process for the NER, Entity Mapping, data structure manipulation and finally data upload steps. Curation tools can be categorised as GUI applications (computer programs and web applications) and CLI applications (interconnected programming tools, libraries and packages) (Figure 3). As an example, multiple page documents can be searched for taxon names in seconds, with technologies that find synonyms and fuzzy search for the OCR transformation misspelling. The interconnection and guidance of these steps still requires human interaction and correction.
GUI applications are standalone applications or web applications, the latter support document upload and, once they are processed in a server, the results are delivered back to the user (Lamurias and Couto, 2019). CLI tools include programming packages and libraries of any programming language in UNIX (Linux and Mac operating systems - OS) and Windows OS. Even though programming packages and libraries are fast and scalable they require familiarity and expertise in CLI and programming which, on the other hand, takes effort and time because of its initial learning curve. The CLI tools, Application Programming Interfaces (APIs) and programming packages chosen during this study are open-source, are in active development, can process many documents and can be combined with other tools in some of the considered steps.


    \subsection{Case Study}
The historical document “Report on the Mollusca and Radiata of the Aegean Sea: and on their Distribution, Considered as Bearing on Geology” by Forbes (1844) and its curated dataset were used as a case study for the tool usage description and evaluation (where applicable). More specifically, the six page long Appendix No. 1 (pages 180-185) document has been manually curated and published, thus serving as a golden standard (Figure 4). It was digitised and transcribed on 2009-04-22 by the Internet Archive17 and on 2021-09-30 it was manually curated (Mavraki et al., 2021) and published in MedOBIS18 (Arvanitidis et al., 2006). The rescue process resulted in a Darwin Core Archive file with 530 occurrence records, 17 unique sampling stations and 260 taxa, covering 217 species. The effort required from the information extraction task to data publishing was roughly 50 working days (8 hours per day) by a single data curator.

    \subsection{Tool Usability Evaluation}
The web applications mentioned in this work were tested in November 2020 in 
two web browsers, Mozilla Firefox version 83 and Google Chrome version 87, both on Microsoft Windows 10 and MacOS 10.14.

    \subsection{Demonstrator}

    DECO was developed for the automation of biodiversity historical data curation. Its workflow combines image processing tools for scanned historical documents OCR with text mining technologies. It extracts biodiversity entities such as taxon names, environments as described in ENVO and tissue mentions. The extracted entities are further enriched with marine data identifiers from public APIs (e.g. WoRMS) and presented in a structured format as well as in report format with automated visualisation components. Furthermore, the workflow was implemented as a Docker container to ease its installation and its scalable application on large documents. DECO is under the GNU GPLv3 licence (for 3rd party components separate licences apply) and is available via the GitHub repository (https://github. com/lab42open-team/deco).


% DECO RESULTS
\section{Results}
\label{sec:deco-results}

   \subsection{Historical Literature Discovery}
   \subsection{Bioinformatics Tools Compilation and Review}

   \subsubsection{Named Entity Recognition}
   \subsubsection{Entity Normalisation and Mapping}
   \subsubsection{Data Transformations}
   \subsubsection{Quality Control}
   \subsubsection{Upload to Database}
   \subsubsection{One-Stop-Shop Tools}
   \subsection{DECO: A Biodiversity Data Curation Programming Workflow}

% DECO DISCUSSION
\section{Discussion}
\label{sec:deco-discussion}

   \subsection{Data Rescue Landscape}
   \subsection{Interface Remarks}
   \subsection{Sustainability}
   \subsection{Curation Step-Wise Remarks}
   \subsection{DECO}
   \subsection{Future Outlook}
   \subsection{Concluding Remarks}
