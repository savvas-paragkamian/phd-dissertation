% --------------------------------------------------
% 
% This chapter is for DECO
% 
% --------------------------------------------------


\chapter{How to accelerate the rescue of historical biodiversity literature?}
\label{cha:deco}


% DECO INTRODUCTION
\section{Introduction}
\label{sec:deco-intro}

The value of numeric representation of ecological and biological data was pioneered by D'Arcy Wentworth Thompson
with his work "On Growth and Form" (1917). Apart from establishing the mathematical biology
he demonstrated that with data is possible to implement conservation policies, 
see \href{https://www.biodiversitylibrary.org/item/63749}{Mission to the Behring Sea in 1896}.
By applying mathematical principles to the study of biological systems,
he helped to develop new methods for understanding the complex interactions within
ecosystems and the impacts of human activities on these systems.
His approach and work are exemplary even by modern standards and showcase that 
numerical ecology and conservation are intertwined. 

The combination of occurrences of species on earth and across timelines provides 
important insights to ecologists as discussed in \textcite{bolanakis2024} and in chapter \ref{cha:arthropods}.
Yet the historical documents and occurrences remain mostly unrepresented in
data platforms, such as the Global Biodiversity Information Facility (GBIF) \parencite{noauthor_gbif_nodate}
as indicated in chapter \ref{cha:crete-idea} and in \textcite{Paragkamian2022}.
The rescue of historical data will be realised when they are transformed to
modern standards \parencite{bowker_biodiversity_2000,Paragkamian2022}.
These documents are under threat as the data holders may
(a) have forgotten these details, (b) be retired or (c) be deceased \parencite{michener_nongeospatial_1997}.

The interpretation of historical information in contemporary standards 
is extremely time consuming. 
Manual curation is a demanding process that requires interdisciplinary 
knowledge and multiple tools.
However, text mining tools have been in use to accelerate the
curation process \parencite{alex_assisted_2008}. Text mining is defined as the
automatic extraction of information from unstructured data
\parencite{hearst_untangling_1999,10.5555/1199003}.
Named Entity Recognition (NER) is the most used step text mining applications
which identifies terms of interest in text \parencite{perera_named_2020}.
Biodiversity document rescue process requires the following entities:

\begin{enumerate}

    \item taxon names,
    \item people’s names \parencite{page_text-mining_2019,groom_people_2020},
    \item environments/ habitats \parencite{pafilis_environments_2015,pafilis_extract_2017},
    \item geolocations/ localities \parencite{alex_adapting_2015,stahlman_geoparsing_2019},
    \item phenotypic traits/morphological characteristics \parencite{thessen_automated_2018}
    \item physico-chemical variables
    \item quantities, measurement units and/or values.

\end{enumerate}

Many tools have emerged with functionality to retrieve a one or multiple 
of the aforementioned entities \parencite{batista-navarro_text_2017,10.3897/BDJ.7.e28737,dimitrova_pensoft_2020,le_guillarme_taxonerd_2022}.
For a comprehensive review see \textcite{Paragkamian2022}.
These can be categorised in Web applications and in Command line interface tools (CLI).
Web apps have visual aids that improve the evaluation and are intuitive because of
the graphical interface.
These applications are useful in most cases but are siloed in functions.
They have many dependencies which increase instability, hence they require more effort to maintain.
CLI tools are scalable and reproducible due to code base flexibility.
They can be executed multiple times with multiple documents.
CLI tools have become more portable and stable with the containerised applications like Docker.
Major disadvantages of CLI tools is that they have steep learning curves
and lack interactiveness, making them cumbersome during the curation process.

In this chapter, the development of a novel CLI tool is presented. This tool serves
as a demonstrator biodiversity data curation workflow. 
It is named DECO (bioDivErsity data Curation programming wOrkflow \footnote{https://github.com/lab42open-team/deco}).
DECO combines multiple curation steps, like Object Character Recognition, Named Entity Recognition, Entity mapping
for various entity types. In addition, DECO is available as a software container 
to increase portability and longevity.


% DECO METHODS
\section{Method}
\label{sec:deco-method}

    \subsection{DECO}

The DECO (bioDivErsity Data Curation wOrkflow) is a command-line interface
designed for biodiversity data rescue. It starts with Optical Character Recognition
(OCR) and proceeds through Named Entity Recognition (NER) and Entity Mapping, Figure \ref{fig:deco-workflow}.
It is suitable for large libraries containing scanned biodiversity documents.
The tool requires a PDF file as input and outputs the contents of the file in a specified directory.
The tool includes usage instructions, requiring parameters for the PDF file
path (-f) and output directory (-d). If these parameters are missing or incorrect, it displays appropriate error messages and exits.
Also the need for a stable internet connection during certain steps is emphasized.

Once the script successfully starts, it creates a new directory for the outputs
and assigns a unique identifier to the files. It converts the PDF pages into images
using ImageMagick, then uses Tesseract for OCR to extract text from these images.
All extracted text is combined into a single text file. Next, DECO performs NER
using the EXTRACT tool to identify organisms, environments, and tissues, and
uses the gnfinder tool to detect organism names. It then maps these entities to Aphia IDs via an API.

Finally, the DECO generates a report using RMarkdown, summarizing the workflow's results and the total runtime of the workflow.
The script operates under the GNU GPLv3 license and emphasizes 

   \begin{figure}[ht]
      \centering
      \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{figures/DECO-workflow.png}
      \caption[DECO workflow]{The steps and tools used in the DECO software container.}
      \label{fig:deco-workflow}
   \end{figure}


    \subsection{Optical Character Recognition}

To perform OCR, the Tesseract 4 OCR engine is employed. 
Prior to performing OCR the provided PDF document must be transformed to images. This 
task is carried out with imageMagick tool.
Then, for each page of the document the Tesseract is executed to generate text.
In cases which the PDF document has been 
already OCRed this step is replaced by ghostscript text extraction from PDF.
    

    \subsection{Named Entity Recognition}
\textit{Global Names Parser} was employed to process scientific names. Gnfinder
is a fast, high-precision tool designed for biodiversity informaticians and
biologists working with large numbers of scientific names.
It can replace manual parsing and standardization methods, which are expensive
and prone to errors, thereby enhancing the interoperability of biological information \parencite{mozzherin_gnamesgnfinder_2022}.
Gnfinder returns a json file that is transformed to tabular data with jq tool.

DECO uses the EXTRACT API script, a Perl-based tool designed for interacting
with the EXTRACT API to identify and tag specific entities within a given text.
The script processes a tab-delimited input file, which contains columns for resource ID,
field, and text to be mined. It sends the text entries to the EXTRACT Tagger API,
retrieves the identified entities, and outputs the results in a tab-delimited format,
listing the tagged words, their entity types, and term identifiers.
This script supports a variety of entity types and DECO uses the NCBI Taxonomy
entries, BRENDA Tissue Ontology terms, and Environment Ontology terms.
The opens the input file processes each entry by making an HTTP POST request to the EXTRACT API.
This script was authored by Dr Evangelos Pafilis of the Hellenic Centre for Marine Research 
and is available with the 2-clause BSD License \parencite{pafilis2016extract}.

    \subsection{Entity Mapping}
    
In DECO there is an R script that defines the functions to interact with the WoRMS (World Register of Marine Species)
API based on different query types and to process the retrieved data.
The worms api function constructs the appropriate API URL and makes GET requests
to the WoRMS API, returning the content if the request is successful or the status code otherwise.
The get AphiaIDs extract function takes a vector of IDs and a tool name,
iteratively querying the WoRMS API for each ID using the specified tool,
storing the results in a dataframe, and including a progress bar to monitor the process.
It avoids server overload by introducing random pauses between requests.
The get AphiaIDs gnfinder function is similar but tailored for the "gnfinder" tool,
handling multiple records per query and appending results to a dataframe.
Both functions return dataframes with standardized column names containing information from the WoRMS database.
Depending on the size of the document and it's contents, this step can be the most 
time consuming because of pauses between requests. For the requests the httr R package was used \parencite{httr2023}.


    \subsection{Reporting}
The RMarkdown script titled "DECO results and Report", automatically generates a
report displaying workflow information and descriptive statistics of the results.
It reads two tab-delimited files containing taxonomic data from gnfinder and extract.
The data from these files are grouped by taxonomic rank and summarized to count the total occurrences for each rank.
Two bar plots are created using ggplot2: the first visualizes the taxonomic ranks
of Aphia IDs found with gnfinder, and the second visualizes the ranks found with EXTRACT.
Both plots display the counts of taxa for each rank. The report also includes placeholders
for the number of document pages and the word count.
Packages used are ggplot2 \parencite{wickham_ggplot2_2016} and rmarkdown \parencite{rmarkdown2020}. 

    \subsection{Container}

The Docker container described is built from an Ubuntu 18.04 base image.
It starts by setting the environment variables to make apt non-interactive,
then installs basic tools like wget and curl. Subsequent steps include updating
the package list and installing essential packages like software-properties-common, git, and setting the timezone to Athens.
For R dependencies, it removes any existing R installations and installs
several required libraries and compilers, including gfortran, build-essential, and aptitude.
System libraries needed for the Tidyverse R package \parencite{Wickham2019tidy} are also installed, alongside the Pandoc document converter.
The container then sets up Tesseract OCR dependencies, which include various
image libraries, followed by building and installing Leptonica from source.
Ghostscript and ImageMagick are downloaded, built, and installed from source as well.
Next, it installs jq for JSON processing and Tesseract OCR itself, along with its
language data files. The container also installs gnfinder for finding scientific names in texts.
R 4.0.3 is downloaded and installed, and the Tidyverse package is added using Rscript.
The second part of the container setup clones the DECO workflow repository from GitHub,
adjusts permissions for the /home/deco directory, and sets it as the working directory when the container starts.

\subsection{Tools}
The system tools of DECO consist of a collection of software, each with
specific versions optimized for various tasks. GNU bash (v3.2.57) and R (v4.0.3)
facilitate scripting and statistical analysis. Perl (v5.32.0) is used for advanced text processing.
Data transformation is handled by tools such as awk (v20070501) for tabular data,
ghostscript (v9.53.3) for PDFs, ImageMagick (v7.0.10-30) for images,
and jq (v1.6) for JSON files.
R packages tidyverse (v1.3.0) and httr (v1.4.2) streamline data handling and web-based data extraction.
Workflow tools comprise Tesseract OCR (v4.1.1) for optical character recognition,
gnfinder (v0.11.1) for document segmentation, and EXTRACT (v2) for extracting data from documents.

% DECO RESULTS
\section{Results and Discussion}
\label{sec:deco-results}

\subsection{Terminology reconciliation}

The literature review and tool evaluation lead to the summarisation and 
reconciliation of curation steps terms, Figure \ref{fig:rescue-workflow}. 
Many works refer to digitisation as imaging and information extraction together. 
It is important to distinguish all steps required in order to be more precise 
and be able to design tools to facilitate such tasks.

Digitisation is the task of bringing a physical document in digital form. 
This is the first task towards the rescue of a document. The steps to 
do this is first to identify the document. Second step is the imaging (e.g scanning)
the document using specific standards, then add various metadata to the collated images,
and finally upload to a database. Most of the times the format is the PDF format of 
the digitised documents.

The second task is the transcription of the imaged document. This task
takes the images of the document and transforms it to text format. The
Optical Character Recognition step is part of computer vision field
which identifies patterns in images and assigns letters. This step is 
a well established field with multiple tools available \parencite{Paragkamian2022}.
Even though there is a lot of praise in these tools, most of the time they 
need corrections especially in languages other than English and in cases of 
complex formatting of documents, e.g tables, special characters etc. For that 
reason is it suggested to be curated and corrected in this task.

The third task is about Information Extraction. This step is what interests 
ecologists the most and the current text mining tools goal. These tools can perform
entity recognition and mapping in identifiers of databases. Some also provide visual 
cues which are useful. The last step is the publication of occurrences and taxa
information to public databases using standards. This form of data is the state of 
the art and only then the information of the documents is considered rescued.

   \begin{figure}[htp!]
      \centering
      \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{figures/deco-figure-1.jpg}
      \caption[Historical document rescue process]{Steps of the historical document rescue (Figure adopted from \parencite{Paragkamian2022}).}
      \label{fig:rescue-workflow}
   \end{figure}

\subsection{Curation across tasks}

Human curation of historical documents is challenging albeit priceless as discussed 
in chapters \ref{cha:crete-idea} and \ref{cha:arthropods}.
It is a tedious and interdisciplinary process \parencite{faulwetter_emodnet_2016,beja_chapter_2022},
that handles propagated errors derived from all the aforementioned tasks.
Georeferencing issues may arise from the use of outdated location names or sampling
points instead of actual geographical coordinates. Furthermore, taxonomic constraints, like synonyms and the lack of referenced taxon names or specimens,
necessitate expertise from taxonomists. Measurements in non-standard units must be
converted to the SI system. Changes in old geographical nomenclature and
political boundaries over time, as well as land-sea coordinate shifts due
to coastline alterations, should be considered. Moreover, the prevalence of
non-English languages in historic scientific papers necessitates curators proficient in multiple languages, 
especially French, German and Latin.

Curators' specialized domain knowledge is irreplaceable by automation.
Numerous historical documents remain uncurated in digital libraries such as BHL,
Belgian Marine Bibliography, Web of Science, Wiley Online Library, and Taylor and
Francis Online, to name a few \parencite{kearney_its_2019}. While BHL offers Optical
Character Recognition (OCR) for these documents, and there are several tools for
this purpose, OCR still represents a significant bottleneck in data processing,
particularly for handwritten texts or poor-quality images \parencite{lyal_digitising_2016}.
Nevertheless, the rapid advancements of tools help curators to perform faster and
more accurate document rescue.


\subsection{A Biodiversity Data Curation Programming Workflow}

It is common that each tool performs one step as summarised in \parencite{Paragkamian2022, beja_chapter_2022}.
The one-stop-shop approach aims at incorporating multiple workflow steps. 
The Golden-GATE-imagine\footnote{\url{https://github.com/plazi/GoldenGATE-Imagine}}, an updated
version of GoldenGATE editor \parencite{sautter_semi-automated_2007}, is such tool. This tool
performs transcription and information extraction and provides annotations on PDF derived from ontologies. It
was developed by Plazi in 2015 and was last updated in 2016, albeit still functional.

The same approach was adopted by the CLI workflow, DECO, developed in this work.
DECO demonstrates the advantages of the CLI tools and open source software.
DECO performs OCR, organism name (with 2 tools) and environments entity recognition, entity mapping 
to WoRMS, and reporting with a summary. It is executed with a single command with 
the PDF and returns each page in text, taxon names per page with NCBI IDs and WoRMS IDS and environments
ENVO IDS. It also returns a file that summarises all steps and entities identifications.
To facilitate all this functionality multiple tools are employed which makes it 
cumbersome for the users to install. This also makes the tool unstable from 
future releases of updates. To resolve these issues DECO is provided as 
a DOCKER container as well. The code and containers have been tested on
Ubuntu, Mac and Debian server (Table \ref{table-CLI}).

Furthermore, DECO, is a demonstration of EMODnet Biology’s aims to rescue historical biodiversity documents.
This is the first CLI tool that brings together state-of-the-art image processing, OCR tools, text mining
technologies and Web APIs, in order to assist curators. By using programming
interface and Command Line Tools the workflow is scalable, customisable and
modular, meaning that more tools can be incorporated to, e.g. include the
entities mentioned in the previous section. It can be used on a
personal computer and on servers as a Docker container or as an executable workflow.

\begin{table}[ht]
    \begin{tabular}{p{0.192\linewidth}p{0.192\linewidth}p{0.192\linewidth}p{0.192\linewidth}p{0.192\linewidth}}
\hline
\textbf{OS} & \textbf{Local running time} & \textbf{Container running time} & \textbf{CPU} & \textbf{RAM (GB)} \\
\hline
macOS Catalina 10.15.7 & 28 min & Docker - 33 min & Intel(R) Core(TM) i5-4258U CPU @ 2.40GHz & 8 \\
Linux Ubuntu 18.04.5 LTS (Bionic Beaver) & 20 min & Docker  27 min & Intel(R) Pentium(R) Dual-Core CPU T4200 @ 2.00GHz & 4 \\
Linux Debian server 4.9.0-8-amd64 & --- & Singularity - 20 min & Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz & 4
\end{tabular}%
\caption{The platforms where the CLI workflow was tested.Please note that running time can be affected by internet speed and stability due to API calls. The workflow uses open source tools and software libraries that are distributed across the major platforms; Linux, Mac and Windows.}
\label{table-CLI}
\end{table}

\section{Conclusion}

The historical biodiversity rescue community is small across the globe and the funding 
is even less. Most people perform these tasks because they care and/or want to
incorporate historical context in their work as shown in chapter \ref{cha:arthropods}. 
Working groups and initiatives in infrastructures are beginning to 
form with Biodiversity Information Standards (TDWG) adoption in historical documents.
Yet a lot of effort is required to rescue the already digitised documents.
In addition, a huge leap is needed to rescue the hard copies of many 
biodiversity documents, which comprise the vase majority of historical 
documents. The Biodiversity Heritage Library and Internet Archives 
have done a tremendous work in this realm with 300 million pages 
imaged but this is the top of the iceberg comparing to what lies in 
scientists libraries and drawers. This is a major challenge of the current 
generation of ecologists, to save the work of previous generations. 
Tools like DECO and GoldenGATE demonstrate the possibilities 
and future directions in applications to accelerate this work.
In my view, these tools and approaches must be provided but big 
infrastructures and/or services because the rapid advancements of 
text mining field, like transformer models and large language models, require 
dedicated developers and funding.
Furthermore, it's essential that the community and funding bodies prioritize the preservation of these irreplaceable records, as they face the risk of deterioration and eventual loss.
