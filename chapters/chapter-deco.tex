% --------------------------------------------------
% 
% This chapter is for DECO
% 
% --------------------------------------------------


\chapter{How to accelerate the rescue of historical biodiversity literature?}
\label{cha:deco}


% DECO INTRODUCTION
\section{Introduction}
\label{sec:deco-intro}

The value of numeric representation of ecological and biological data was pioneered by D'Arcy Wentworth Thompson
with his work "On Growth and Form" (1917). Apart from establishing the mathematical biology
he demonstrated that with data is possible to implement conservation policies, 
see \href{https://www.biodiversitylibrary.org/item/63749}{Mission to the Behring Sea in 1896}.
By applying mathematical principles to the study of biological systems,
he helped to develop new methods for understanding the complex interactions within
ecosystems and the impacts of human activities on these systems.
His approach and work are exemplary even by modern standards and showcase that 
numerical ecology and conservation are intertwined. 

The combination of occurrences of species on earth and across timelines provides 
important insights to ecologists as discussed in \textcite{bolanakis2024} and in chapter \ref{cha:arthropods}.
Yet the historical documents and occurrences remain mostly unrepresented in
data platforms, such as the Global Biodiversity Information Facility (GBIF) \parencite{noauthor_gbif_nodate}
as indicated in chapter \ref{cha:crete-idea} and in \textcite{Paragkamian2022}.
The rescue of historical data will be realised when they are transformed to
modern standards \parencite{bowker_biodiversity_2000,Paragkamian2022}.
These documents are under threat as the data holders may
(a) have forgotten these details, (b) be retired or (c) be deceased \parencite{michener_nongeospatial_1997}.

The interpretation of historical information in contemporary standards 
is extremely time consuming. 
Manual curation is a demanding process that requires interdisciplinary 
knowledge and multiple tools.
However, text mining tools have been in use to accelerate the
curation process \parencite{alex_assisted_2008}. Text mining is defined as the
automatic extraction of information from unstructured data
\parencite{hearst_untangling_1999,10.5555/1199003}.
Named Entity Recognition (NER) is the most used step text mining applications
which identifies terms of interest in text \parencite{perera_named_2020}.
Biodiversity document rescue process requires the following entities:

\begin{enumerate}

    \item taxon names,
    \item people’s names \parencite{page_text-mining_2019,groom_people_2020},
    \item environments/ habitats \parencite{pafilis_environments_2015,pafilis_extract_2017},
    \item geolocations/ localities \parencite{alex_adapting_2015,stahlman_geoparsing_2019},
    \item phenotypic traits/morphological characteristics \parencite{thessen_automated_2018}
    \item physico-chemical variables
    \item quantities, measurement units and/or values.

\end{enumerate}

Many tools have emerged with functionality to retrieve a one or multiple 
of the aforementioned entities \parencite{batista-navarro_text_2017,10.3897/BDJ.7.e28737,dimitrova_pensoft_2020,le_guillarme_taxonerd_2022}.
For a comprehensive review see \textcite{Paragkamian2022}.
These can be categorised in Web applications and in Command line interface tools (CLI).
Web apps have visual aids that improve the evaluation and are intuitive because of
the graphical interface.
These applications are useful in most cases but are siloed in functions.
They have many dependencies which increase instability, hence they require more effort to maintain.
CLI tools are scalable and reproducible due to code base flexibility.
They can be executed multiple times with multiple documents.
CLI tools have become more portable and stable with the containerised applications like Docker.
Major disadvantages of CLI tools is that they have steep learning curves
and lack interactiveness, making them cumbersome during the curation process.

In this chapter, the development of a novel CLI tool is presented. This tool serves
as a demonstrator biodiversity data curation workflow. 
It is named DECO (bioDivErsity data Curation programming wOrkflow \footnote{https://github.com/lab42open-team/deco}).
DECO combines multiple curation steps, like Object Character Recognition, Named Entity Recognition, Entity mapping
for various entity types. In addition, DECO is available as a software container 
to increase portability and longevity.


% DECO METHODS
\section{Method}
\label{sec:deco-method}

\subsection{General workflow}


   \begin{figure}[htp!]
      \centering
      \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{figures/deco-figure-1.jpg}
      \caption[Historical document rescue process]{Steps of the historical document rescue (Figure adopted from \parencite{Paragkamian2022}).
      Availability of information from historical data increases as the curation tasks are completed (as exemplified by the fan on the right).
  Icons used from the Noun Project released under CC BY: book by Oleksandr Panasovskyi, scanning by LAFS, Book info by Xinh Studio, Library by ibrandify, Scanner Text by Wolf Böse, Check form by allex, Whale by Alina Oleynik, Fish by Asmuh, tag code vigorn, pivot layout by paisan, Certificate by P Thanga Vignesh, web service by mynamepong.}
      \label{fig:rescue-workflow}
   \end{figure}


    \subsection{Demonstrator}

DECO was developed for the automation of biodiversity historical data
curation. Its workflow combines image processing tools for scanned historical
documents OCR with text mining technologies, Figure \ref{fig:deco-workflow}. It extracts biodiversity entities
such as taxon names, environments as described in ENVO and tissue mentions.
The extracted entities are further enriched with marine data identifiers from
public APIs (e.g. WoRMS) and presented in a structured format as well as in
report format with automated visualisation components. Furthermore, the
workflow was implemented as a Docker container to ease its installation and its
scalable application on large documents. DECO is under the GNU GPLv3 licence
(for 3rd party components separate licences apply) and is available via the
GitHub repository (\url{https://github.com/lab42open-team/deco}).

DECO is available here:\url{https://github.com/lab42open-team/deco}. Historical
marine literature analysis is here:
\href{https://github.com/savvas-paragkamian/historical-marine-literature}{https://github.com/savvas-paragkamian/historical-marine-literature}.
BHL, EMODnet Biology and OBIS data are available for download here
\href{https://about.biodiversitylibrary.org/tools-andservices/developer-and-data-tools/}{https://about.biodiversitylibrary.org/tools-andservices/developer-and-data-tools/}
and \url{https://www.emodnetbiology.eu/toolbox/en/download/occurrence/explore}
and here \url{https://obis.org/manual/access/}, respectively. The digitised
document of the “Report on the Mollusca andRadiata of the Aegean Sea, and on
their distribution, considered as bearing on Geology. 13th Meeting of the
British Association for the Advancement of Science, London, 1844” is available
here: \url{https://www.biodiversitylibrary.org/page/12920789}. The curated
dataset of the case study is available here (version 1.9 and above):
\url{http://ipt.medobis.eu/resource?r=mollusca_forbes}.

   \begin{figure}[ht]
      \centering
      \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{figures/DECO-workflow.png}
      \caption[DECO workflow]{The steps and tools used in the DECO software container.}
      \label{fig:deco-workflow}
   \end{figure}

% DECO RESULTS
\section{Results}
\label{sec:deco-results}



   \subsubsection{One-Stop-Shop Tools}
The main all-in-one GUI computer program is Golden-GATE-
imagine\footnote{\url{https://github.com/plazi/GoldenGATE-Imagine}}, an updated
version of GoldenGATE editor \parencite{sautter_semi-automated_2007}. This tool
supports OCR, NER and entity mapping, as described in the various steps of the
curator’s workflow by providing annotations on PDF backed up by ontologies. It
was developed by Plazi in 2015 and was last updated in 2016. Several recent
biodiversity data related publications still report the use of it although it
has not been updated since that time
\parencite{10.3897/biss.3.37078,rivera-quiroz_extracting_2019,10.3897/biss.4.59178}.
Due to its open source nature, Golden-Gate-imagine can be further developed by
any interested parties, as exemplified in GNfinder.

\begin{table}[ht]
\large
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllll}
\hline
\textbf{OS} & \textbf{Source code - running time} & \textbf{Container - running time (minutes)} & \textbf{CPU} & \textbf{RAM (GB)} \\
\hline
macOS Catalina 10.15.7 & 28 minutes & Docker - 33’ & Intel(R) Core(TM) i5-4258U CPU @ 2.40GHz & 8 \\
Linux Ubuntu 18.04.5 LTS (Bionic Beaver) & 20 minutes & Docker - 27’ & Intel(R) Pentium(R) Dual-Core CPU T4200 @ 2.00GHz & 4 \\
Linux Debian server 4.9.0-8-amd64 & --- & Singularity - 20’ & Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz & 4
\end{tabular}%
}
\caption{The platforms where the CLI workflow was tested.Please note that running time can be affected by internet speed and stability due to API calls. The workflow uses open source tools and software libraries that are distributed across the major platforms; Linux, Mac and Windows.}
\label{table-CLI}
\end{table}

   \subsection{DECO: A Biodiversity Data Curation Programming Workflow}
A CLI workflow named DECO developed to demonstrate the advantages of the CLI
approach, is available via this GitHub repository\footnote{\url{https://github.com/lab42open-team/deco}}.
DECO has connected different tools of the programming curation steps
\ref{fig:curation-process}. The execution is via a single command with a
user-provided PDF file and the output are the taxon names and records from
WoRMS API, taxonomy NCBI IDs and ENVO terms from the Environmental Ontology.
Complementary tools (i.e. Ghostscript\footnote{https://www.ghostscript.com/index.html},
jq\footnote{https://stedolan.github.io/jq/} and ImageMagick\footnote{https://imagemagick.org/index.php})
and UNIX commands are also called in a single Bash script which unifies
the workflow. In order to simplify the setup procedure of the workflow a Docker
container and a Singularity container were developed that include all the
dependencies and the code. The code and both containers have been tested on
Ubuntu, Mac and Debian server (Table \ref{table-CLI}). For a larger corpus of biodiversity
historical data the recommendation is to use the Singularity container in a
remote server or a High Performance Computing (HPC) cluster.

% DECO DISCUSSION
\section{Discussion}
\label{sec:deco-discussion}

Several factors may turn the curation of historical documents into a
serious challenge \parencite{faulwetter_emodnet_2016,beja_chapter_2022}.
Errors from the first and second tasks, as presented in Figure 
\ref{fig:rescue-workflow} (i.e. bad quality imaging, mis-recognised characters etc.)
are propagated through the whole process. In terms of georeferencing
constraints, location names or sampling points on an old map may be provided
instead of the actual coordinates. Additionally, taxonomic constraints
(e.g. old, currently unaccepted synonyms, lack of authority associated with the
taxon names) combined with the absence of taxonomic literature or voucher
specimens (e.g. identifier number for samples of natural history/expedition
collections) require the taxonomists’ assistance. Numerical measurement units
often need to be converted to the International System of Units (SI system)
(e.g. fathoms to metres)\parencite{calder_proposal_1982,wieczorek_darwin_2012}.
Old toponyms and political boundaries that have now changed should also be
taken into consideration, as well as coordinates that now fall on land instead
of in the sea, due to the changes in the coastline. Lastly, the use of
languages other than English is quite common in old scientific publications, so
multilingual curators are required. 

   \subsection{Data Rescue Landscape}
   The huge difference between rescued historical marine datasets uploaded on
OBIS and the available digital items on BHL holdings reflects the challenges
faced by curators and the minimal attention paid by the wider community,
when compared to other data rescue activities (e.g. specimens, oceanographic
data, etc.). Many publications lack basic metadata such as location, date,
purpose or method of sampling. Tracing this information is limited as the data
providers may (a) have forgotten these details, (b) be retired or (c) be
deceased \parencite{michener_nongeospatial_1997}.

In the last few years, an upsurge in web applications development regarding the
enhancement of biodiversity data digitisation has been observed. This is an
indication of the need for such initiatives. Advancements in the field of OCR,
text mining and information technology promise semi-automation and acceleration
of the curator’s work, which could transform the biodiversity curation field
into an -omics like, interdisciplinary field that requires complementary skills
of document handling, web technologies and text mining, to name but a few.

   \subsection{Curation Step-Wise Remarks}
   The curators’ role is invaluable in the data rescue process, as their domain
specific expertise is far from becoming entirely automated. There are plenty of
available digitised historical documents that are not curated in web libraries,
such as BHL, the Belgian Marine
Bibliography\footnote{\url{https://www.vliz.be/en/belgian-marine-bibliography}},
Web of Science \footnote{https://www.webofknowledge.com}, Wiley Online
Library \footnote{https://onlinelibrary.wiley.com} and Taylor \& Francis
Online \footnote{https://www.tandfonline.com/}, among others \parencite{kearney_its_2019}.
BHL provides “OCRed” documents and there are plenty of other tools that can
tackle this process which are reviewed elsewhere \parencite{10.3897/rio.6.e58030},
however OCR is a crucial limiting step in the workflows, in terms of the
information transformed from image to text, because there are many cases that
lead to mispelled or lost text; especially the case with handwritten text and
poor quality images \parencite{lyal_digitising_2016}.
   
\subsection{DECO}
   The CLI scientific workflow assembled in this paper, DECO, is a demonstration
of EMODnet Biology’s vision for biodiversity data rescue using programming
tools. To the best of our knowledge, this is the first task-driven CLI that
brings together state-of-the-art image processing, OCR tools, text mining
technologies and Web APIs, in order to assist curators. By using programming
interface and Command Line Tools the workflow is scalable, customisable and
modular, meaning that more tools can be incorporated to, e.g. include the
entities mentioned in the previous section. It is fast, may be used on a
personal computer, and is available as a Docker and a Singularity container.
The containerised versions of the workflow simplify the installation procedure
and increase its stability, scalability and portability because they include
all the necessary dependencies. This CLI scientific workflow promises a faster
and high throughput processing that could be applied to any type of data, not
only historical, thus contributing to the overall digitisation of biodiversity
knowledge.

   \subsection{Concluding Remarks}
   Historical marine biodiversity data provide important and significant
snapshots of the past that can help understand the current status of ocean
ecosystems and predict future trends in face of the climate crisis. There is a
wealth of historical documents that have been digitised yet, most of their data
have not been rescued or published in online systems. To accelerate the tedious
data rescue process it is essential that more curators become engaged, and
tools for Information Extraction and Publication get improved to satisfy their
needs. Tools like DECO and GoldenGATE demonstrate possible future directions
for one-stop-shop applications for command line and graphical user interfaces,
respectively. Research Infrastructures can play a pivotal role towards this
goal. Last but not least, the community and funding bodies should prioritise
the data rescue of these invaluable documents before their decay and inevitable
loss.
